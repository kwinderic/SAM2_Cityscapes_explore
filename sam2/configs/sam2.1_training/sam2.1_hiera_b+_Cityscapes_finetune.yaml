mode: train_only
max_epochs: 40
accelerator: cuda
seed_value: 123

scratch:
  resolution: 1024
  train_batch_size: 8
  num_train_workers: 10
  base_lr: 5.0e-6
  vision_lr: 3.0e-06
  num_epochs: 40

model:
  _target_: training.model.sam2.SAM2Train
  image_encoder:
    _target_: sam2.modeling.backbones.image_encoder.ImageEncoder
    scalp: 1
    trunk:
      _target_: sam2.modeling.backbones.hieradet.Hiera
      embed_dim: 112
      num_heads: 2
      drop_path_rate: 0.1
    neck:
      _target_: sam2.modeling.backbones.image_encoder.FpnNeck
      position_encoding:
        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine
        num_pos_feats: 256
        normalize: true
        scale: null
        temperature: 10000
      d_model: 256
      backbone_channel_list: [896, 448, 224, 112]
      fpn_top_down_levels: [2, 3]
      fpn_interp_model: nearest

  image_size: 1024  
  # use high-resolution feature map in the SAM mask decoder
  use_high_res_features_in_sam: true
  # SAM heads
  iou_prediction_use_sigmoid: True
  ####### Training specific params #######
  # box/point input and corrections
  prob_to_use_pt_input_for_train: 0.5
  prob_to_use_pt_input_for_eval: 0.0
  prob_to_use_box_input_for_train: 0.5  # 0.5*0.5 = 0.25 prob to use box instead of points
  prob_to_use_box_input_for_eval: 0.0
  prob_to_sample_from_gt_for_train: 0.1  # with a small prob, sampling correction points from GT mask instead of prediction errors
  num_frames_to_correct_for_train: 2  # iteratively sample on random 1~2 frames (always include the first frame)
  num_frames_to_correct_for_eval: 1  # only iteratively sample on first frame
  rand_frames_to_correct_for_train: True  # random #init-cond-frame ~ 2
  add_all_frames_to_correct_as_cond: True  # when a frame receives a correction click, it becomes a conditioning frame (even if it's not initially a conditioning frame)
  # maximum 2 initial conditioning frames
  num_init_cond_frames_for_train: 2
  rand_init_cond_frames_for_train: True  # random 1~2
  num_correction_pt_per_frame: 7
  use_act_ckpt_iterative_pt_sampling: false
  

  
  num_init_cond_frames_for_eval: 1  # only mask on the first frame
  forward_backbone_per_frame_for_eval: True


dataset:
  img_folder: /mnt/d/study/2024autumn/AutonomousDV/dataset/leftImg8bit  # Update this path
  gt_folder: /mnt/d/study/2024autumn/AutonomousDV/dataset/gtFine  # Update this path
  file_list_txt: null  # Optional

vos:
  train_transforms:
    - _target_: training.dataset.transforms.ComposeAPI
      transforms:
        - _target_: training.dataset.transforms.RandomHorizontalFlip
        - _target_: training.dataset.transforms.RandomAffine
          degrees: 25
          shear: 20
          image_interpolation: bilinear
        - _target_: training.dataset.transforms.RandomResizeAPI
          sizes: 1024
          square: true
        - _target_: training.dataset.transforms.ColorJitter
          brightness: 0.1
          contrast: 0.03
          saturation: 0.03
        - _target_: training.dataset.transforms.RandomGrayscale
          p: 0.05
        - _target_: training.dataset.transforms.ToTensorAPI
        - _target_: training.dataset.transforms.NormalizeAPI
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]

launcher:
  num_nodes: 1
  gpus_per_node: 8
  experiment_log_dir: /exp/sam2.1_hiera_b+_Cityscapes_finetune  # Update this path

trainer:
  _target_: training.trainer.Trainer
  mode: train_only
  max_epochs: 40
  accelerator: cuda
  seed_value: 123

  model:
    _target_: training.model.sam2.SAM2Train
    image_encoder:
      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder
      scalp: 1
      trunk:
        _target_: sam2.modeling.backbones.hieradet.Hiera
        embed_dim: 112
        num_heads: 2
        drop_path_rate: 0.1
      neck:
        _target_: sam2.modeling.backbones.image_encoder.FpnNeck
        position_encoding:
          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine
          num_pos_feats: 256
          normalize: true
          scale: null
          temperature: 10000
        d_model: 256
        backbone_channel_list: [896, 448, 224, 112]
        fpn_top_down_levels: [2, 3]
        fpn_interp_model: nearest

  data:
    train:
      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset
      batch_sizes:
        - ${scratch.train_batch_size}
      datasets:
        - _target_: training.dataset.vos_dataset.VOSDataset
          transforms: ${vos.train_transforms}
          training: true
          video_dataset:
            _target_: training.dataset.vos_raw_dataset.PNGRawDataset
            img_folder: ${dataset.img_folder}
            gt_folder: ${dataset.gt_folder}
            file_list_txt: ${dataset.file_list_txt}
      shuffle: True
      num_workers: ${scratch.num_train_workers}
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: training.utils.data_utils.collate_fn
        _partial_: true
        dict_key: all

  logging:
    tensorboard_writer:
      _target_: training.utils.logger.make_tensorboard_logger
      log_dir:  /exp/sam2.1_hiera_b+_Cityscapes_finetune/tensorboard
      flush_secs: 120
      should_log: True
    log_dir: /exp/sam2.1_hiera_b+_Cityscapes_finetune/logs
    log_freq: 10

  checkpoint:
    save_dir: /exp/sam2.1_hiera_b+_Cityscapes_finetune/checkpoints
    save_freq: 0 # 0 only last checkpoint is saved.
    model_weight_initializer:
      _partial_: True
      _target_: training.utils.checkpoint_utils.load_state_dict_into_model
      strict: True
      ignore_unexpected_keys: null
      ignore_missing_keys: null

      state_dict:
        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels
        checkpoint_path: ./checkpoints/sam2.1_hiera_base_plus.pt # PATH to SAM 2.1 checkpoint
        ckpt_state_dict_keys: ['model']

  optim:
    amp:
      enabled: True
      amp_dtype: bfloat16

    optimizer:
      _target_: torch.optim.AdamW

    gradient_clip:
      _target_: training.optimizer.GradientClipper
      max_norm: 0.1
      norm_type: 2

    param_group_modifiers:
      - _target_: training.optimizer.layer_decay_param_modifier
        _partial_: True
        layer_decay_value: 0.9
        apply_to: 'image_encoder.trunk'
        overrides:
          - pattern: '*pos_embed*'
            value: 1.0

    options:
      lr:
        - scheduler:
          _target_: torch.optim.lr_scheduler.StepLR
          step_size: 10
          gamma: 0.1



submitit:
  partition: null
  account: null
  qos: null
  cpus_per_task: 10
  use_cluster: false
  timeout_hour: 24
  name: null
  port_range: [10000, 65000]